{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ Unsloth å¯¹ DeepSeek-R1-Distill-Qwen-14B æ¨¡å‹è¿›è¡Œ LoRA å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:30:54.074191Z",
     "iopub.status.busy": "2025-08-31T13:30:54.073486Z",
     "iopub.status.idle": "2025-08-31T13:30:54.077237Z",
     "shell.execute_reply": "2025-08-31T13:30:54.076732Z",
     "shell.execute_reply.started": "2025-08-31T13:30:54.074153Z"
    }
   },
   "source": [
    "### 1. ç¯å¢ƒå‡†å¤‡ä¸åº“å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:47:11.354734Z",
     "iopub.status.busy": "2025-08-31T13:47:11.354516Z",
     "iopub.status.idle": "2025-08-31T13:47:11.358071Z",
     "shell.execute_reply": "2025-08-31T13:47:11.357554Z",
     "shell.execute_reply.started": "2025-08-31T13:47:11.354713Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T13:29:46.360862Z",
     "start_time": "2025-08-31T13:29:36.063611Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-31T13:47:11.358865Z",
     "iopub.status.busy": "2025-08-31T13:47:11.358666Z",
     "iopub.status.idle": "2025-08-31T13:47:21.310935Z",
     "shell.execute_reply": "2025-08-31T13:47:21.310313Z",
     "shell.execute_reply.started": "2025-08-31T13:47:11.358845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/deepseek-quickstart/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ (Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:47:21.313069Z",
     "iopub.status.busy": "2025-08-31T13:47:21.312369Z",
     "iopub.status.idle": "2025-08-31T13:47:34.862282Z",
     "shell.execute_reply": "2025-08-31T13:47:34.861675Z",
     "shell.execute_reply.started": "2025-08-31T13:47:21.313045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.5: Fast Qwen2 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰æ¨¡å‹å’Œä¸€äº›åŸºæœ¬å‚æ•°\n",
    "max_seq_length = 8192\n",
    "dtype = None # None è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹© (Float16 a T4, V100, BFloat16 a Ampere)\n",
    "load_in_4bit = True # ä½¿ç”¨ 4bit é‡åŒ–åŠ è½½\n",
    "\n",
    "# è¿™æ˜¯æ‚¨çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼Œè¯·æ›¿æ¢ä¸ºæ‚¨æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹\n",
    "# ä¾‹å¦‚ï¼š\"qwen-1.5b_lora_model\"\n",
    "# model_name = \"qwen-1.5b_lora_model\" \n",
    "# model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\" \n",
    "# model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\" \n",
    "model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit\" \n",
    "\n",
    "# è¿™ä¸€æ­¥ä¼šè¿”å›ä¸€ä¸ªç»è¿‡ Unsloth ä¼˜åŒ–çš„æ¨¡å‹å’Œä¸€ä¸ªåˆ†è¯å™¨\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. å¾®è°ƒå‰æ¨ç†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:47:34.863092Z",
     "iopub.status.busy": "2025-08-31T13:47:34.862886Z",
     "iopub.status.idle": "2025-08-31T13:47:34.866128Z",
     "shell.execute_reply": "2025-08-31T13:47:34.865634Z",
     "shell.execute_reply.started": "2025-08-31T13:47:34.863073Z"
    }
   },
   "outputs": [],
   "source": [
    "# æ¨¡å‹æ¨ç†çš„ Prompt æ¨¡æ¿\n",
    "inference_prompt = \"\"\"ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\n",
    "è¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\n",
    "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\n",
    "\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\n",
    "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:47:34.867079Z",
     "iopub.status.busy": "2025-08-31T13:47:34.866707Z",
     "iopub.status.idle": "2025-08-31T13:48:23.323174Z",
     "shell.execute_reply": "2025-08-31T13:48:23.322573Z",
     "shell.execute_reply.started": "2025-08-31T13:47:34.867059Z"
    }
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "question = \"ç”·ï¼Œ28å²ï¼Œç¨‹åºå‘˜ï¼Œæœ€è¿‘ä¸€å‘¨æ¯å¤©å·¥ä½œåˆ°åŠå¤œï¼Œæ„Ÿè§‰å¤´æ™•ã€è„–å­ç–¼ï¼Œæœ‰æ—¶å€™è¿˜æ¶å¿ƒã€‚\"\n",
    "\n",
    "inputs = tokenizer([inference_prompt.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "attention_mask = inputs.input_ids.ne(tokenizer.pad_token_id).long().to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:48:23.324465Z",
     "iopub.status.busy": "2025-08-31T13:48:23.323886Z",
     "iopub.status.idle": "2025-08-31T13:48:23.327948Z",
     "shell.execute_reply": "2025-08-31T13:48:23.327446Z",
     "shell.execute_reply.started": "2025-08-31T13:48:23.324444Z"
    }
   },
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:48:23.328923Z",
     "iopub.status.busy": "2025-08-31T13:48:23.328520Z",
     "iopub.status.idle": "2025-08-31T13:48:23.339068Z",
     "shell.execute_reply": "2025-08-31T13:48:23.338574Z",
     "shell.execute_reply.started": "2025-08-31T13:48:23.328903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "å—¯ï¼Œæˆ‘ç°åœ¨è¦å¤„ç†ä¸€ä¸ªå…³äº28å²ç¨‹åºå‘˜çš„å¥åº·é—®é¢˜ã€‚ä»–æœ€è¿‘ä¸€å‘¨æ¯å¤©å·¥ä½œåˆ°åŠå¤œï¼Œæ„Ÿè§‰å¤´æ™•ã€è„–å­ç–¼ï¼Œæœ‰æ—¶å€™è¿˜æ¶å¿ƒã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦åˆ†æè¿™äº›ç—‡çŠ¶å¯èƒ½çš„åŸå› ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œå¤´æ™•å’Œè„–å­ç–¼å¯èƒ½ä¸é•¿æ—¶é—´çš„å·¥ä½œå§¿åŠ¿æœ‰å…³ã€‚ç¨‹åºå‘˜é€šå¸¸ä¹…åï¼Œå¯èƒ½å§¿åŠ¿ä¸è‰¯ï¼Œå¯¼è‡´è‚Œè‚‰ç–²åŠ³æˆ–é¢ˆæ¤é—®é¢˜ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œå¯èƒ½è¯Šæ–­ä¸ºé¢ˆéƒ¨è‚Œè‚‰åŠ³æŸæˆ–é¢ˆæ¤ç—…ã€‚\n",
      "\n",
      "å…¶æ¬¡ï¼Œæ¶å¿ƒå¯èƒ½ä¸é¢ˆæ¤é—®é¢˜æœ‰å…³ï¼Œå› ä¸ºé¢ˆæ¤ç—…æœ‰æ—¶ä¼šå¼•èµ·æ¤åŠ¨è„‰å‹è¿«ï¼Œå½±å“ä¾›è¡€ï¼Œå¯¼è‡´å¤´æ™•å’Œæ¶å¿ƒã€‚ä¹Ÿæœ‰å¯èƒ½æ˜¯å…¶ä»–åŸå› ï¼Œæ¯”å¦‚è¡€å‹å¼‚å¸¸ã€è´«è¡€æˆ–å†…è€³é—®é¢˜ï¼Œä½†ç»“åˆä»–çš„å·¥ä½œä¹ æƒ¯ï¼Œé¢ˆæ¤é—®é¢˜æ›´æœ‰å¯èƒ½ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦è€ƒè™‘å…¶ä»–å¯èƒ½çš„åŸå› ã€‚æ¯”å¦‚ï¼Œé•¿æ—¶é—´ä½¿ç”¨ç”µå­è®¾å¤‡å¯èƒ½å¯¼è‡´çœ¼ç›ç–²åŠ³ï¼Œä½†é€šå¸¸ä¸ä¼šå¼•èµ·ä¸¥é‡çš„å¤´æ™•å’Œæ¶å¿ƒã€‚å¦å¤–ï¼Œå‹åŠ›å¤§å¯èƒ½å¯¼è‡´ç´§å¼ æ€§å¤´ç—›ï¼Œä½†è¿™ä¹Ÿé€šå¸¸ä¸ä¼´æœ‰æ¶å¿ƒï¼Œé™¤éæ˜¯ä¸¥é‡çš„åå¤´ç—›ã€‚\n",
      "\n",
      "ç„¶åï¼Œæˆ‘åº”è¯¥å»ºè®®ä»–è¿›è¡Œä¸€äº›åˆæ­¥çš„è‡ªæˆ‘æŠ¤ç†æªæ–½ï¼Œæ¯”å¦‚æ”¹å–„å·¥ä½œå§¿åŠ¿ï¼Œé€‚å½“ä¼‘æ¯ï¼Œåšä¸€äº›é¢ˆéƒ¨æ‹‰ä¼¸è¿åŠ¨ã€‚åŒæ—¶ï¼Œå»ºè®®ä»–å‡å°‘ç”µå­è®¾å¤‡çš„ä½¿ç”¨ï¼Œä¿æŒè‰¯å¥½çš„ç”Ÿæ´»ä¹ æƒ¯ï¼Œå¦‚è§„å¾‹ä½œæ¯å’Œé€‚é‡è¿åŠ¨ã€‚\n",
      "\n",
      "å¦‚æœè¿™äº›ç—‡çŠ¶æŒç»­æˆ–åŠ é‡ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥çš„åŒ»ç–—è¯„ä¼°ã€‚æ¯”å¦‚ï¼Œè¿›è¡Œé¢ˆæ¤çš„Xå…‰æˆ–MRIæ£€æŸ¥ï¼Œæ’é™¤æ¤é—´ç›˜çªå‡ºæˆ–å…¶ä»–ç»“æ„æ€§é—®é¢˜ã€‚åŒæ—¶ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥è¡€å‹å’Œè¡€ç³–ï¼Œæ’é™¤ç³–å°¿ç—…æˆ–é«˜è¡€å‹çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "åœ¨æ²»ç–—æ–¹é¢ï¼Œç‰©ç†æ²»ç–—å¯èƒ½æœ‰åŠ©äºç¼“è§£é¢ˆéƒ¨è‚Œè‚‰ç´§å¼ ï¼Œæ”¹å–„è¡€æ¶²å¾ªç¯ã€‚å¦‚æœæœ‰å¿…è¦ï¼Œå¯ä»¥è€ƒè™‘è¯ç‰©æ²»ç–—ï¼Œå¦‚éç”¾ä½“æŠ—ç‚è¯æ¥ç¼“è§£ç–¼ç—›å’Œç‚ç—‡ã€‚æ­¤å¤–ï¼Œå¿ƒç†å‹åŠ›ç®¡ç†ä¹Ÿæ˜¯é‡è¦çš„ä¸€ç¯ï¼Œå¯èƒ½å»ºè®®ä»–è¿›è¡Œæ”¾æ¾è®­ç»ƒæˆ–å¯»æ±‚å¿ƒç†è¾…å¯¼ã€‚\n",
      "\n",
      "æ€»çš„æ¥è¯´ï¼Œè¿™ä¸ªç—…ä¾‹ä¸»è¦è€ƒè™‘é¢ˆæ¤é—®é¢˜çš„å¯èƒ½æ€§è¾ƒå¤§ï¼Œä½†ä¹Ÿä¸èƒ½å®Œå…¨æ’é™¤å…¶ä»–åŸå› ï¼Œå› æ­¤å»ºè®®ä»–åŠæ—¶å°±åŒ»è¿›è¡Œè¯¦ç»†æ£€æŸ¥ï¼Œä»¥è·å¾—å‡†ç¡®çš„è¯Šæ–­å’Œé€‚å½“çš„æ²»ç–—ã€‚\n",
      "</think>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ä¸‹è½½å’Œæ ¼å¼åŒ–è®­ç»ƒæ•°æ®é›†\n",
    "\n",
    "åŒ»å­¦æ¨ç†æ•°æ®é›†ï¼šhttps://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT/viewer/zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:48:23.339822Z",
     "iopub.status.busy": "2025-08-31T13:48:23.339625Z",
     "iopub.status.idle": "2025-08-31T13:48:23.354261Z",
     "shell.execute_reply": "2025-08-31T13:48:23.353764Z",
     "shell.execute_reply.started": "2025-08-31T13:48:23.339805Z"
    }
   },
   "outputs": [],
   "source": [
    "# æ¨¡å‹è®­ç»ƒçš„ Prompt æ¨¡æ¿\n",
    "train_prompt = \"\"\"ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\n",
    "è¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\n",
    "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\n",
    "\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\n",
    "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:48:23.355233Z",
     "iopub.status.busy": "2025-08-31T13:48:23.355021Z",
     "iopub.status.idle": "2025-08-31T13:48:27.069603Z",
     "shell.execute_reply": "2025-08-31T13:48:27.069051Z",
     "shell.execute_reply.started": "2025-08-31T13:48:23.355216Z"
    }
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # æ·»åŠ  EOS Token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        # å°† EOS Token æ·»åŠ åˆ°æ ·æœ¬æœ€å\n",
    "        text = train_prompt.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"zh\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:48:27.070606Z",
     "iopub.status.busy": "2025-08-31T13:48:27.070258Z",
     "iopub.status.idle": "2025-08-31T13:48:27.076722Z",
     "shell.execute_reply": "2025-08-31T13:48:27.076221Z",
     "shell.execute_reply.started": "2025-08-31T13:48:27.070586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\\nè¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\\nåœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\\n\\n\\n### Instruction:\\nä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\\nè¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\\n\\n### Question:\\næ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ\\n\\n### Response:\\n<think>\\nè¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\\n\\nç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\\n\\nä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\\n\\nç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\\n\\nä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\\n\\nå›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\\n\\nå†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\\n\\nçœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\\n\\nä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\\n\\nå“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚\\n</think>\\nä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\\n<ï½œendâ–ofâ–sentenceï½œ>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:48:27.077649Z",
     "iopub.status.busy": "2025-08-31T13:48:27.077333Z",
     "iopub.status.idle": "2025-08-31T13:48:27.084153Z",
     "shell.execute_reply": "2025-08-31T13:48:27.083681Z",
     "shell.execute_reply.started": "2025-08-31T13:48:27.077630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\n",
       "è¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\n",
       "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\n",
       "\n",
       "\n",
       "### Instruction:\n",
       "ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\n",
       "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n",
       "\n",
       "### Question:\n",
       "æ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ\n",
       "\n",
       "### Response:\n",
       "<think>\n",
       "è¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\n",
       "\n",
       "ç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\n",
       "\n",
       "ä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\n",
       "\n",
       "ç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\n",
       "\n",
       "ä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\n",
       "\n",
       "å›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\n",
       "\n",
       "å†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\n",
       "\n",
       "çœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\n",
       "\n",
       "ä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\n",
       "\n",
       "å“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚\n",
       "</think>\n",
       "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
       "<ï½œendâ–ofâ–sentenceï½œ>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(dataset[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ä½¿ç”¨ Unsloth æ·»åŠ  LoRA é€‚é…å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:48:27.086253Z",
     "iopub.status.busy": "2025-08-31T13:48:27.085939Z",
     "iopub.status.idle": "2025-08-31T13:48:33.709166Z",
     "shell.execute_reply": "2025-08-31T13:48:33.708607Z",
     "shell.execute_reply.started": "2025-08-31T13:48:27.086233Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.5 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
      "        (layers): ModuleList(\n",
      "          (0-4): 5 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "          )\n",
      "          (5-6): 2 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "          )\n",
      "          (7-22): 16 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "          )\n",
      "          (23): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "          )\n",
      "          (24-25): 2 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "          )\n",
      "          (26): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "          )\n",
      "          (27-42): 16 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "          )\n",
      "          (43-46): 4 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "          )\n",
      "          (47): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# å› ä¸º `model` å¯¹è±¡ç°åœ¨æ˜¯ç”± Unsloth åˆ›å»ºçš„ï¼Œå®ƒåŒ…å«äº†æ‰€æœ‰å¿…éœ€çš„å±æ€§\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "      \"q_proj\",\n",
    "      \"k_proj\",\n",
    "      \"v_proj\",\n",
    "      \"o_proj\",\n",
    "      \"gate_proj\",\n",
    "      \"up_proj\",\n",
    "      \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=1432,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "# æ£€æŸ¥æ¨¡å‹ç»“æ„ï¼Œç¡®è®¤ LoRA é€‚é…å™¨å·²æ·»åŠ \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. é…ç½® SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:48:33.710312Z",
     "iopub.status.busy": "2025-08-31T13:48:33.710068Z",
     "iopub.status.idle": "2025-08-31T13:48:34.132498Z",
     "shell.execute_reply": "2025-08-31T13:48:34.131948Z",
     "shell.execute_reply.started": "2025-08-31T13:48:33.710292Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.119, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 32,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 1432,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:48:34.133602Z",
     "iopub.status.busy": "2025-08-31T13:48:34.133161Z",
     "iopub.status.idle": "2025-08-31T14:18:56.290668Z",
     "shell.execute_reply": "2025-08-31T14:18:56.290024Z",
     "shell.execute_reply.started": "2025-08-31T13:48:34.133582Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 2 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 68,812,800 of 14,838,846,464 (0.46% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 28 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (28 x 2 x 1) = 56\n",
      " \"-____-\"     Trainable parameters = 68,812,800 of 14,838,846,464 (0.46% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 25 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (25 x 2 x 1) = 50\n",
      " \"-____-\"     Trainable parameters = 68,812,800 of 14,838,846,464 (0.46% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 22 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (22 x 2 x 1) = 44\n",
      " \"-____-\"     Trainable parameters = 68,812,800 of 14,838,846,464 (0.46% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 19 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (19 x 2 x 1) = 38\n",
      " \"-____-\"     Trainable parameters = 68,812,800 of 14,838,846,464 (0.46% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 17 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (17 x 2 x 1) = 34\n",
      " \"-____-\"     Trainable parameters = 68,812,800 of 14,838,846,464 (0.46% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 15 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (15 x 2 x 1) = 30\n",
      " \"-____-\"     Trainable parameters = 68,812,800 of 14,838,846,464 (0.46% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 13 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (13 x 2 x 1) = 26\n",
      " \"-____-\"     Trainable parameters = 68,812,800 of 14,838,846,464 (0.46% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 28:37, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.707900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.320500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.844800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.657600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.680200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.771800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.775400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.671600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.630300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.680300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.546200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.569300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.589600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.618700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.668900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.541700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.719600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.629200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.609500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.646500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.627900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.637800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.608100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.614800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.559300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.583900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=60, training_loss=1.7653941671053568, metrics={'train_runtime': 1746.496, 'train_samples_per_second': 0.893, 'train_steps_per_second': 0.034, 'total_flos': 1.1465029713993523e+17, 'train_loss': 1.7653941671053568})\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n",
    "\n",
    "# æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯\n",
    "print(trainer_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹ï¼ˆLoraï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:21:33.575409Z",
     "iopub.status.busy": "2025-08-31T14:21:33.575084Z",
     "iopub.status.idle": "2025-08-31T14:21:35.330144Z",
     "shell.execute_reply": "2025-08-31T14:21:35.329557Z",
     "shell.execute_reply.started": "2025-08-31T14:21:33.575388Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"qwen-14b_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:21:35.331523Z",
     "iopub.status.busy": "2025-08-31T14:21:35.330886Z",
     "iopub.status.idle": "2025-08-31T14:21:35.456978Z",
     "shell.execute_reply": "2025-08-31T14:21:35.456433Z",
     "shell.execute_reply.started": "2025-08-31T14:21:35.331493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qwen-14b_lora_model/tokenizer_config.json',\n",
       " 'qwen-14b_lora_model/special_tokens_map.json',\n",
       " 'qwen-14b_lora_model/chat_template.jinja',\n",
       " 'qwen-14b_lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"qwen-14b_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:21:35.457854Z",
     "iopub.status.busy": "2025-08-31T14:21:35.457645Z",
     "iopub.status.idle": "2025-08-31T14:21:35.460758Z",
     "shell.execute_reply": "2025-08-31T14:21:35.460118Z",
     "shell.execute_reply.started": "2025-08-31T14:21:35.457836Z"
    }
   },
   "outputs": [],
   "source": [
    "# æ¨¡å‹ä¿å­˜æ–¹å¼äºŒé€‰ä¸€ï¼ˆè¦ä¹ˆä½¿ç”¨ä¸Šé¢çš„åˆ†å¼€ä¿å­˜ï¼Œè¦ä¹ˆä½¿ç”¨è¿™é‡Œçš„åˆå¹¶ Lora ä¿å­˜ï¼‰\n",
    "# model.save_pretrained_merged(\"qwen-14b_lora_model\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. æµ‹è¯•è®­ç»ƒåçš„ç”Ÿæˆç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:21:35.461768Z",
     "iopub.status.busy": "2025-08-31T14:21:35.461434Z",
     "iopub.status.idle": "2025-08-31T14:22:17.379629Z",
     "shell.execute_reply": "2025-08-31T14:22:17.378999Z",
     "shell.execute_reply.started": "2025-08-31T14:21:35.461749Z"
    }
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "question=\"ä¸€ä¸ªæ‚£æœ‰æ€¥æ€§é˜‘å°¾ç‚çš„ç—…äººå·²ç»å‘ç—…5å¤©ï¼Œè…¹ç—›ç¨æœ‰å‡è½»ä½†ä»ç„¶å‘çƒ­ï¼Œåœ¨ä½“æ£€æ—¶å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ï¼Œæ­¤æ—¶åº”å¦‚ä½•å¤„ç†ï¼Ÿ\", # Question\n",
    "inputs = tokenizer([inference_prompt.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1000,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:22:17.381013Z",
     "iopub.status.busy": "2025-08-31T14:22:17.380446Z",
     "iopub.status.idle": "2025-08-31T14:22:17.384981Z",
     "shell.execute_reply": "2025-08-31T14:22:17.384441Z",
     "shell.execute_reply.started": "2025-08-31T14:22:17.380989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "å—¯ï¼Œè¿™ä½ç—…äººå·²ç»æ‚£æ€¥æ€§é˜‘å°¾ç‚5å¤©äº†ï¼Œè…¹ç—›è™½ç„¶ç¨å¾®å‡è½»ï¼Œä½†è¿˜æ˜¯å‘çƒ§å‘¢ã€‚è¿™å¯ä¸å¤ªå¯»å¸¸ï¼Œé€šå¸¸æ€¥æ€§é˜‘å°¾ç‚åœ¨å‘ç—…24å°æ—¶å†…ä¼šå¼€å§‹å¥½è½¬ï¼Œä½†è¿™ä½ç—…äººå´åœ¨5å¤©åè¿˜åœ¨å‘çƒ§ï¼Œè¯´æ˜å¯èƒ½æœ‰å¹¶å‘ç—‡ã€‚å†æƒ³æƒ³ï¼Œé˜‘å°¾ç‚å¦‚æœé•¿æ—¶é—´ä¸å¤„ç†ï¼Œå¯èƒ½ä¼šå‘å±•æˆé˜‘å°¾ç©¿å­”ï¼Œè¿™æ ·å¯å°±éº»çƒ¦äº†ã€‚\n",
      "\n",
      "å“¦ï¼Œå¯¹äº†ï¼Œç—…äººåœ¨ä½“æ£€æ—¶å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ã€‚è¿™å¯æ˜¯ä¸ªå±é™©çš„ä¿¡å·ï¼Œå¾ˆå¯èƒ½é˜‘å°¾å·²ç»ç©¿å­”äº†ï¼Œå¯¼è‡´è…¹è†œç‚ã€‚è…¹è†œç‚çš„è¯ï¼Œéœ€è¦ç«‹å³å¤„ç†ï¼Œå¦åˆ™ä¼šå¾ˆå±é™©ã€‚\n",
      "\n",
      "æ‰€ä»¥ï¼Œè¿™ä¸ªæ—¶å€™ï¼Œæˆ‘è§‰å¾—åº”è¯¥èµ¶ç´§å®‰æ’æ‰‹æœ¯ã€‚æ‰‹æœ¯æ˜¯å¤„ç†æ€¥æ€§é˜‘å°¾ç‚å¹¶å‘ç—‡æœ€ç›´æ¥çš„åŠæ³•ã€‚å½“ç„¶ï¼Œæ‰‹æœ¯å‰è¿˜å¾—åšäº›å‡†å¤‡ï¼Œæ¯”å¦‚å®Œå–„ä¸€äº›æ£€æŸ¥ï¼Œæ¯”å¦‚è¡€å¸¸è§„ã€è…¹éƒ¨è¶…å£°ï¼Œçœ‹çœ‹æƒ…å†µåˆ°åº•æœ‰å¤šä¸¥é‡ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œæ‰‹æœ¯ä¹‹å‰ï¼Œæ˜¯ä¸æ˜¯å¾—å…ˆçœ‹çœ‹æœ‰æ²¡æœ‰å…¶ä»–å¯èƒ½æ€§å‘¢ï¼Ÿæ¯”å¦‚é˜‘å°¾ç‚ç©¿å­”ä¹‹å‰æœ‰æ²¡æœ‰å…¶ä»–ç—‡çŠ¶ï¼Œæˆ–è€…æ˜¯ä¸æ˜¯å…¶ä»–åœ°æ–¹å‡ºäº†é—®é¢˜ã€‚è¿™äº›æ£€æŸ¥æœ‰åŠ©äºç¡®è®¤è¯Šæ–­ï¼Œç¡®ä¿æ‰‹æœ¯çš„å¿…è¦æ€§ã€‚\n",
      "\n",
      "å¦‚æœç¡®å®è¯Šæ–­ä¸ºé˜‘å°¾ç©¿å­”ï¼Œé‚£æ‰‹æœ¯å°±å˜å¾—å¾ˆç´§è¿«äº†ã€‚æ¯•ç«Ÿï¼Œè…¹è†œç‚å¦‚æœä¸åŠæ—¶å¤„ç†ï¼Œå¾ˆå¿«å°±ä¼šå±åŠç”Ÿå‘½ã€‚\n",
      "\n",
      "å½“ç„¶ï¼Œæ‰‹æœ¯åè¿˜å¾—æ³¨æ„æ¢å¤ã€‚æœ¯åå¾—å¥½å¥½æŠ¤ç†ï¼Œé˜²æ­¢æ„ŸæŸ“ï¼Œè®©ç—…äººå°½å¿«åº·å¤ã€‚æ€»ä¹‹ï¼Œé¢å¯¹è¿™ç§å¤æ‚æƒ…å†µï¼ŒåŠæ—¶è€Œå‡†ç¡®çš„å¤„ç†æ˜¯å…³é”®ã€‚\n",
      "</think>\n",
      "åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç—…äººå·²ç»æ‚£æ€¥æ€§é˜‘å°¾ç‚5å¤©ï¼Œè…¹ç—›ç¨æœ‰å‡è½»ä½†ä»ç„¶å‘çƒ­ï¼Œä¸”åœ¨ä½“æ£€æ—¶å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ã€‚è¿™äº›ç—‡çŠ¶æç¤ºå¯èƒ½å­˜åœ¨é˜‘å°¾ç©¿å­”å¯¼è‡´çš„è…¹è†œç‚ã€‚\n",
      "\n",
      "åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåº”è¯¥ç«‹å³å®‰æ’æ‰‹æœ¯æ²»ç–—ï¼Œä»¥é˜²æ­¢ç—…æƒ…è¿›ä¸€æ­¥æ¶åŒ–ã€‚æ‰‹æœ¯æ˜¯å¤„ç†æ€¥æ€§é˜‘å°¾ç‚å¹¶å‘ç—‡æœ€ç›´æ¥çš„æ–¹æ³•ã€‚åœ¨æ‰‹æœ¯å‰ï¼Œå»ºè®®è¿›è¡Œä¸€äº›å¿…è¦çš„æ£€æŸ¥ï¼Œå¦‚è¡€å¸¸è§„å’Œè…¹éƒ¨è¶…å£°ï¼Œä»¥ç¡®è®¤è¯Šæ–­ï¼Œç¡®ä¿æ‰‹æœ¯çš„å¿…è¦æ€§ã€‚\n",
      "\n",
      "å¦‚æœç¡®å®è¯Šæ–­ä¸ºé˜‘å°¾ç©¿å­”ï¼Œæ‰‹æœ¯å°±å˜å¾—éå¸¸ç´§è¿«ï¼Œå› ä¸ºè…¹è†œç‚å¦‚æœä¸åŠæ—¶å¤„ç†ï¼Œä¼šè¿…é€Ÿå±åŠç”Ÿå‘½ã€‚æ‰‹æœ¯åï¼Œéœ€è¦ä¸¥æ ¼éµå®ˆæœ¯åæŠ¤ç†æªæ–½ï¼Œé˜²æ­¢æ„ŸæŸ“ï¼Œå¹¶å¸®åŠ©ç—…äººå°½å¿«åº·å¤ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œé¢å¯¹è¿™ç§æƒ…å†µï¼ŒåŠæ—¶å’Œå‡†ç¡®çš„å¤„ç†æ˜¯å…³é”®ï¼Œä»¥ç¡®ä¿ç—…äººçš„å®‰å…¨å’Œå¥åº·ã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(output[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:22:28.390583Z",
     "iopub.status.busy": "2025-08-31T14:22:28.390032Z",
     "iopub.status.idle": "2025-08-31T14:22:28.395728Z",
     "shell.execute_reply": "2025-08-31T14:22:28.395177Z",
     "shell.execute_reply.started": "2025-08-31T14:22:28.390557Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(question: str, model, tokenizer, inference_prompt: str, max_new_tokens: int = 1024) -> str:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹å’Œåˆ†è¯å™¨ä¸ºç»™å®šçš„åŒ»å­¦é—®é¢˜ç”Ÿæˆå“åº”ã€‚\n",
    "\n",
    "    Args:\n",
    "        question (str): éœ€è¦æ¨¡å‹å›ç­”çš„åŒ»å­¦é—®é¢˜ã€‚\n",
    "        model: å·²åŠ è½½çš„ Unsloth/Hugging Face æ¨¡å‹ã€‚\n",
    "        tokenizer: å¯¹åº”çš„åˆ†è¯å™¨ã€‚\n",
    "        inference_prompt (str): ç”¨äºæ ¼å¼åŒ–è¾“å…¥çš„ f-string æ¨¡æ¿ã€‚\n",
    "        max_new_tokens (int, optional): ç”Ÿæˆå“åº”çš„æœ€å¤§ token æ•°é‡ã€‚é»˜è®¤ä¸º 1024ã€‚\n",
    "\n",
    "    Returns:\n",
    "        str: æ¨¡å‹ç”Ÿæˆçš„å“åº”æ–‡æœ¬ï¼Œå·²å»é™¤ prompt éƒ¨åˆ†ã€‚\n",
    "    \"\"\"\n",
    "    # 1. ä½¿ç”¨æ¨¡æ¿æ ¼å¼åŒ–è¾“å…¥\n",
    "    prompt = inference_prompt.format(\n",
    "        question, # å¡«å……é—®é¢˜\n",
    "        \"\",       # ç•™ç©ºï¼Œè®©æ¨¡å‹ç”Ÿæˆ CoT å’Œ Response\n",
    "    )\n",
    "\n",
    "    # 2. å°†æ ¼å¼åŒ–åçš„ prompt è¿›è¡Œåˆ†è¯ï¼Œå¹¶è½¬ç§»åˆ° GPU\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # 3. ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè¾“å‡º\n",
    "    # use_cache=True ç”¨äºåŠ é€Ÿè§£ç è¿‡ç¨‹\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    \n",
    "    # 4. å°†ç”Ÿæˆçš„ token è§£ç ä¸ºæ–‡æœ¬\n",
    "    # skip_special_tokens=True ä¼šç§»é™¤åƒ EOS_TOKEN è¿™æ ·çš„ç‰¹æ®Šæ ‡è®°\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    # 5. åˆ‡åˆ†å­—ç¬¦ä¸²ï¼Œåªè¿”å› \"### Response:\" ä¹‹åçš„éƒ¨åˆ†\n",
    "    # ä½¿ç”¨ .split() åˆ†å‰²å¹¶è·å–å“åº”å†…å®¹ï¼Œ.strip() ç”¨äºå»é™¤å¯èƒ½å­˜åœ¨çš„å‰åç©ºç™½å­—ç¬¦\n",
    "    response_part = decoded_output.split(\"### Response:\")\n",
    "    if len(response_part) > 1:\n",
    "        return response_part[1].strip()\n",
    "    else:\n",
    "        # å¦‚æœæ¨¡å‹æ²¡æœ‰ç”Ÿæˆ \"### Response:\" æ ‡è®°ï¼Œåˆ™è¿”å›æ•´ä¸ªç”Ÿæˆå†…å®¹ä»¥ä¾›è°ƒè¯•\n",
    "        return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:22:29.385415Z",
     "iopub.status.busy": "2025-08-31T14:22:29.384854Z",
     "iopub.status.idle": "2025-08-31T14:23:36.329079Z",
     "shell.execute_reply": "2025-08-31T14:23:36.328469Z",
     "shell.execute_reply.started": "2025-08-31T14:22:29.385390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== æ¨¡å‹å›ç­” ====================\n",
      "<think>\n",
      "å—¯ï¼Œè¿™ä¸ªæ‚£è€…60å²ï¼Œæ˜¯ç”·æ€§ï¼Œä»–å‡ºç°äº†å³ä¾§èƒ¸ç–¼ï¼Œè€Œä¸”åœ¨Xçº¿æ£€æŸ¥ä¸­å‘ç°å³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±äº†ã€‚è¿™è®©æˆ‘æƒ³åˆ°ï¼Œä»–å¯èƒ½æœ‰èƒ¸è…”ç§¯æ¶²ï¼Œè€Œä¸”Xçº¿æ˜¾ç¤ºè‚‹è†ˆè§’æ¶ˆå¤±ï¼Œè¿™é€šå¸¸æç¤ºæœ‰ç§¯æ¶²ï¼Œå› ä¸ºç§¯æ¶²ä¼šæŠŠè‚‹è†ˆè§’ç»™é®ç›–äº†ã€‚\n",
      "\n",
      "æ‰€ä»¥ï¼Œå…ˆæ¥æƒ³æƒ³ï¼Œè¿™å¯èƒ½æ˜¯ä»€ä¹ˆåŸå› å¼•èµ·çš„ã€‚èƒ¸è…”ç§¯æ¶²çš„åŸå› æœ‰å¾ˆå¤šç§ï¼Œæ¯”å¦‚è‚ºç»“æ ¸ã€è‚ºç‚ã€å¿ƒåŠ›è¡°ç«­ã€è‚¿ç˜¤ç­‰ã€‚ä¸è¿‡ï¼Œç»“åˆæ‚£è€…çš„å¹´é¾„å’Œèƒ¸è…”ç§¯æ¶²ï¼Œè‚ºç»“æ ¸æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¸¸è§çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡å®éªŒå®¤æ£€æŸ¥æ¥è¿›ä¸€æ­¥æ˜ç¡®èƒ¸æ°´çš„æ€§è´¨ã€‚é€šå¸¸ï¼Œèƒ¸æ°´çš„æ€§è´¨å¯ä»¥é€šè¿‡ä¸€äº›ç‰¹å®šçš„å®éªŒå®¤æ£€æŸ¥æ¥åˆ¤æ–­ï¼Œæ¯”å¦‚èƒ¸æ°´å¸¸è§„åˆ†æã€ç”ŸåŒ–æ£€æŸ¥ï¼Œä»¥åŠç»†èŒå­¦æ£€æŸ¥ç­‰ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œèƒ¸æ°´å¸¸è§„åˆ†æå¯ä»¥å¸®åŠ©æˆ‘ä»¬äº†è§£èƒ¸æ°´çš„ç»†èƒæ•°ç›®ã€é¢œè‰²ã€ç²˜ç¨ åº¦ç­‰åŸºæœ¬ä¿¡æ¯ã€‚è¿™äº›ä¿¡æ¯å¯ä»¥å¸®åŠ©æˆ‘ä»¬åˆæ­¥åˆ¤æ–­èƒ¸æ°´çš„ç±»å‹ï¼Œæ¯”å¦‚æ˜¯æ¸—å‡ºæ¶²è¿˜æ˜¯æ¼å‡ºæ¶²ï¼Œè¿˜æœ‰ç»†èƒç±»å‹ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œå¯¹äºè‚ºç»“æ ¸æ¥è¯´ï¼Œèƒ¸æ°´é€šå¸¸ä¼šæœ‰ä¸€äº›ç‰¹ç‚¹ï¼Œæ¯”å¦‚ç»†èƒæ•°ç›®å¯èƒ½å¢å¤šï¼Œç»†èƒåˆ†ç±»ä¸­å¯èƒ½ä»¥æ·‹å·´ç»†èƒä¸ºä¸»ã€‚ä½†æ˜¯ï¼Œå•å‡­å¸¸è§„åˆ†æå¯èƒ½è¿˜ä¸å¤Ÿï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šçš„ä¿¡æ¯æ¥ç¡®è®¤ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œç”ŸåŒ–æ£€æŸ¥å¯ä»¥è¿›ä¸€æ­¥æä¾›çº¿ç´¢ã€‚æ¯”å¦‚ï¼Œèƒ¸æ°´ä¸­çš„è›‹ç™½å«é‡ã€ä¹³é…¸è„±æ°¢é…¶ï¼ˆLDHï¼‰æ°´å¹³ã€pHå€¼ç­‰ã€‚è¿™äº›æŒ‡æ ‡å¯ä»¥å¸®åŠ©æˆ‘ä»¬åˆ¤æ–­èƒ¸æ°´çš„æ€§è´¨ï¼Œæ¯”å¦‚æ˜¯ç‚ç—‡æ€§è¿˜æ˜¯æ„ŸæŸ“æ€§ã€‚\n",
      "\n",
      "å¯¹äºè‚ºç»“æ ¸ï¼Œèƒ¸æ°´çš„è›‹ç™½å«é‡é€šå¸¸ä¼šå‡é«˜ï¼Œè€ŒLDHæ°´å¹³ä¹Ÿå¯èƒ½å‡é«˜ï¼Œè¿™æç¤ºèƒ¸æ°´å¯èƒ½ä¸ç‚ç—‡æˆ–æ„ŸæŸ“æœ‰å…³ã€‚æ­¤å¤–ï¼Œèƒ¸æ°´çš„pHå€¼é€šå¸¸ä¼šé™ä½ï¼Œè¿™å¯èƒ½ä¸æ„ŸæŸ“æˆ–ç‚ç—‡æœ‰å…³ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œè¿™äº›ä¿¡æ¯è™½ç„¶æœ‰å¸®åŠ©ï¼Œä½†è¿˜ä¸èƒ½å®Œå…¨ç¡®å®šèƒ¸æ°´çš„æ€§è´¨ï¼Œå°¤å…¶æ˜¯å¯¹äºè‚ºç»“æ ¸æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦æ›´å…·ä½“çš„è¯æ®ã€‚\n",
      "\n",
      "å†æ¥çœ‹ç»†èŒå­¦æ£€æŸ¥ï¼Œæ¯”å¦‚èƒ¸æ°´çš„ç»†èŒåŸ¹å…»å’ŒæŠ—é…¸æŸ“è‰²ã€‚å¦‚æœèƒ¸æ°´ä¸­æœ‰ç»“æ ¸æ†èŒï¼Œé‚£ä¹ˆè¿™äº›æ£€æŸ¥ä¼šæ˜¾ç¤ºé˜³æ€§ï¼Œè¿™å¯ä»¥ç¡®è®¤æ˜¯è‚ºç»“æ ¸å¯¼è‡´çš„èƒ¸æ°´ã€‚\n",
      "\n",
      "ä½†æ˜¯ï¼Œç»†èŒåŸ¹å…»éœ€è¦æ—¶é—´ï¼Œè€Œä¸”é˜³æ€§ç‡å¯èƒ½ä¸æ˜¯ç™¾åˆ†ä¹‹ç™¾ï¼Œå°¤å…¶æ˜¯å½“èƒ¸æ°´ä¸­çš„ç»“æ ¸æ†èŒæ•°é‡è¾ƒå°‘æ—¶ã€‚\n",
      "\n",
      "æ‰€ä»¥ï¼Œç»¼åˆæ¥çœ‹ï¼Œä¸ºäº†å¿«é€Ÿä¸”æœ‰æ•ˆåœ°ç¡®è®¤èƒ¸æ°´çš„æ€§è´¨ï¼Œå°¤å…¶æ˜¯å¯¹äºè‚ºç»“æ ¸ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ç»“åˆå…¶ä»–æ£€æŸ¥ã€‚æ¯”å¦‚ï¼Œèƒ¸æ°´çš„æŠ—é…¸æŸ“è‰²å¯ä»¥å¿«é€Ÿæ£€æµ‹ç»“æ ¸æ†èŒçš„å­˜åœ¨ï¼Œè¿™å¯¹äºç¡®è®¤èƒ¸æ°´çš„æ€§è´¨éå¸¸æœ‰å¸®åŠ©ã€‚\n",
      "\n",
      "æ€»çš„æ¥è¯´ï¼Œå°½ç®¡èƒ¸æ°´å¸¸è§„åˆ†æå’Œç”ŸåŒ–æ£€æŸ¥æä¾›äº†åˆæ­¥çš„ä¿¡æ¯ï¼Œä½†èƒ¸æ°´çš„æŠ—é…¸æŸ“è‰²å¯èƒ½åœ¨ç¡®è®¤èƒ¸æ°´æ€§è´¨æ–¹é¢æ›´æœ‰å¸®åŠ©ï¼Œå°¤å…¶æ˜¯åœ¨è‚ºç»“æ ¸çš„è¯Šæ–­ä¸­ã€‚\n",
      "</think>\n",
      "å¯¹äºè¿™å60å²ç”·æ€§æ‚£è€…ï¼Œå³ä¾§èƒ¸ç–¼å’ŒXçº¿æ£€æŸ¥æ˜¾ç¤ºå³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±ï¼Œåˆæ­¥è€ƒè™‘ä¸ºè‚ºç»“æ ¸ä¼´å³ä¾§èƒ¸è…”ç§¯æ¶²ã€‚è¦ç¡®è®¤èƒ¸æ°´çš„æ€§è´¨ï¼Œå°¤å…¶æ˜¯å¯¹äºè‚ºç»“æ ¸çš„è¯Šæ–­ï¼ŒæŠ—é…¸æŸ“è‰²æ£€æŸ¥æ˜¯ä¸€ä¸ªéå¸¸æœ‰å¸®åŠ©çš„å®éªŒå®¤æ£€æµ‹æ‰‹æ®µã€‚æŠ—é…¸æŸ“è‰²å¯ä»¥å¿«é€Ÿæ£€æµ‹å‡ºèƒ¸æ°´ä¸­çš„ç»“æ ¸æ†èŒï¼Œè¿™å¯¹äºç¡®è®¤èƒ¸æ°´çš„æ€§è´¨ä»¥åŠè‚ºç»“æ ¸çš„è¯Šæ–­å…·æœ‰é‡è¦æ„ä¹‰ã€‚\n",
      "\n",
      "é™¤æ­¤ä¹‹å¤–ï¼Œèƒ¸æ°´çš„å¸¸è§„åˆ†æå’Œç”ŸåŒ–æ£€æŸ¥ä¹Ÿèƒ½æä¾›ä¸€å®šçš„ä¿¡æ¯ï¼Œå¦‚èƒ¸æ°´çš„è›‹ç™½å«é‡å’ŒLDHæ°´å¹³çš„å‡é«˜å¯èƒ½æç¤ºç‚ç—‡æˆ–æ„ŸæŸ“ã€‚ç„¶è€Œï¼Œè¿™äº›æ£€æŸ¥é€šå¸¸ä¸èƒ½ç›´æ¥ç¡®è®¤ç»“æ ¸æ†èŒçš„å­˜åœ¨ã€‚å› æ­¤ï¼Œç»“åˆæŠ—é…¸æŸ“è‰²æ£€æŸ¥ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°ç¡®è®¤èƒ¸æ°´çš„æ€§è´¨å’Œç—…å› ã€‚\n",
      "\n",
      "ç»¼ä¸Šæ‰€è¿°ï¼Œå¯¹äºè‚ºç»“æ ¸ä¼´èƒ¸è…”ç§¯æ¶²çš„è¯Šæ–­ï¼ŒæŠ—é…¸æŸ“è‰²æ£€æŸ¥æ˜¯æœ€èƒ½å¸®åŠ©ç¡®è®¤èƒ¸æ°´æ€§è´¨çš„å®éªŒå®¤æ£€æŸ¥ã€‚\n"
     ]
    }
   ],
   "source": [
    "my_question = \"å¯¹äºä¸€å60å²ç”·æ€§æ‚£è€…ï¼Œå‡ºç°å³ä¾§èƒ¸ç–¼å¹¶åœ¨Xçº¿æ£€æŸ¥ä¸­æ˜¾ç¤ºå³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±ï¼Œè¯Šæ–­ä¸ºè‚ºç»“æ ¸ä¼´å³ä¾§èƒ¸è…”ç§¯æ¶²ï¼Œè¯·é—®å“ªä¸€é¡¹å®éªŒå®¤æ£€æŸ¥å¯¹äº†è§£èƒ¸æ°´çš„æ€§è´¨æ›´æœ‰å¸®åŠ©ï¼Ÿ\"\n",
    "\n",
    "response = generate_response(my_question, model, tokenizer, inference_prompt)\n",
    "print(\"==================== æ¨¡å‹å›ç­” ====================\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:23:36.330499Z",
     "iopub.status.busy": "2025-08-31T14:23:36.330032Z",
     "iopub.status.idle": "2025-08-31T14:24:31.155721Z",
     "shell.execute_reply": "2025-08-31T14:24:31.155098Z",
     "shell.execute_reply.started": "2025-08-31T14:23:36.330478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== æ¨¡å‹å›ç­” ====================\n",
      "<think>\n",
      "å—¯ï¼Œè¿™ä¸ª28å²çš„ç¨‹åºå‘˜ï¼Œä»–å¸¸å¹´ç†¬å¤œï¼Œå·¥ä½œå‹åŠ›ä¹Ÿå¤§ï¼Œçªç„¶å¤´æ™•ç›®çœ©ï¼Œè¿˜æœ‰ç‚¹æ¶å¿ƒï¼Œè¿™å¯ä¸å¤ªå¯»å¸¸ã€‚é¦–å…ˆï¼Œæˆ‘å¾—è€ƒè™‘ä¸€ä¸‹ä»–çš„ç”Ÿæ´»ä¹ æƒ¯ã€‚ç†¬å¤œè‚¯å®šæ˜¯ä¸ªå¤§é—®é¢˜ï¼Œé•¿æœŸç¡çœ ä¸è¶³å¯èƒ½ä¼šå½±å“ä»–çš„èº«ä½“æœºèƒ½ï¼Œå¯¼è‡´è¡€å‹å¼‚å¸¸æˆ–è€…ç²¾ç¥çŠ¶æ€ä¸ä½³ã€‚\n",
      "\n",
      "å†æƒ³æƒ³ï¼Œç¨‹åºå‘˜é€šå¸¸ä¹…åä¸åŠ¨ï¼Œè¿™å¯¹è¡€æ¶²å¾ªç¯å¯ä¸å¤ªå¥½ã€‚çªç„¶ç«™èµ·æ¥å¤´æ™•ï¼Œè¿™è®©æˆ‘æƒ³åˆ°å¯èƒ½æ˜¯ä½è¡€å‹ï¼Œæˆ–è€…æ˜¯ä½“ä½æ€§ä½è¡€å‹ã€‚ä½è¡€å‹çš„è¯ï¼Œé€šå¸¸æ˜¯å› ä¸ºè¡€æ¶²å¾ªç¯ä¸å¥½ï¼Œå¯¼è‡´èº«ä½“ä¾›è¡€ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç«™èµ·çš„æ—¶å€™ï¼Œè¿™ä¼šè®©äººæ„Ÿè§‰å¤´æ™•ã€‚\n",
      "\n",
      "è¿˜æœ‰ï¼Œä»–çš„æ¶å¿ƒç—‡çŠ¶è®©æˆ‘æƒ³åˆ°å¯èƒ½æ˜¯ä¸­æš‘ï¼Œä½†é€šå¸¸ä¸­æš‘æ˜¯å¤å¤©åœ¨é«˜æ¸©ä¸‹æ‰ä¼šå‡ºç°ï¼Œè€Œä»–ç°åœ¨çš„æƒ…å†µå¯èƒ½ä¸æ˜¯ä¸­æš‘ã€‚ä¹Ÿæœ‰å¯èƒ½æ˜¯å†…è€³çš„é—®é¢˜ï¼Œæ¯”å¦‚æ¢…å°¼åŸƒç—…ï¼Œè¿™ç§ç—…ä¼šå¯¼è‡´å¤´æ™•å’Œæ¶å¿ƒï¼Œä½†é€šå¸¸ä¼šæœ‰å¬åŠ›ä¸‹é™ï¼Œè€Œä»–å¹¶æ²¡æœ‰æåˆ°å¬åŠ›é—®é¢˜ã€‚\n",
      "\n",
      "å†æƒ³æƒ³ï¼Œä»–å¯èƒ½è¿˜æœ‰é¢ˆæ¤ç—…ï¼Œå› ä¸ºé•¿æœŸçš„ä¹…åå’Œä¸æ­£ç¡®çš„åå§¿ï¼Œéƒ½å¯èƒ½å¯¼è‡´é¢ˆæ¤é—®é¢˜ï¼Œè¿™æ ·å¤´æ™•å’Œæ¶å¿ƒä¹Ÿæ˜¯æœ‰å¯èƒ½çš„ã€‚ä¸è¿‡ï¼Œé€šå¸¸é¢ˆæ¤ç—…ä¼šæœ‰æ›´å¤šçš„ç—‡çŠ¶ï¼Œæ¯”å¦‚é¢ˆéƒ¨åƒµç¡¬æˆ–è€…æ”¾å°„æ€§ç–¼ç—›ï¼Œä»–å¹¶æ²¡æœ‰æåˆ°è¿™äº›ã€‚\n",
      "\n",
      "è¿˜æœ‰ï¼Œä»–æœ€è¿‘æœ‰æ²¡æœ‰ä»€ä¹ˆæƒ…ç»ªä¸Šçš„é—®é¢˜å‘¢ï¼Ÿå‹åŠ›å¤§ã€å·¥ä½œç´§å¼ å¯èƒ½ä¼šå¼•å‘ç„¦è™‘ï¼Œè€Œç„¦è™‘æœ‰æ—¶å€™ä¹Ÿä¼šå¯¼è‡´å¤´æ™•å’Œæ¶å¿ƒã€‚å—¯ï¼Œè¿™ä¼¼ä¹æ˜¯ä¸ªå¯èƒ½çš„åŸå› ã€‚\n",
      "\n",
      "å†æƒ³æƒ³ï¼Œä»–çš„å¤´æ™•ç—‡çŠ¶æ˜¯ä¸æ˜¯åœ¨ç«™èµ·çš„æ—¶å€™æ›´æ˜æ˜¾ï¼Ÿå¦‚æœæ˜¯è¿™æ ·ï¼Œé‚£å¯èƒ½å°±æ˜¯ä½“ä½æ€§ä½è¡€å‹ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œäººç«™èµ·æ—¶è¡€å‹çªç„¶ä¸‹é™ï¼Œå¯¼è‡´ä¾›è¡€ä¸è¶³ï¼Œå°±ä¼šå¤´æ™•ç”šè‡³æ™•å¥ã€‚\n",
      "\n",
      "ç»¼åˆæ¥çœ‹ï¼Œä»–çš„ç—‡çŠ¶æ›´ç¬¦åˆä½è¡€å‹æˆ–è€…ä½“ä½æ€§ä½è¡€å‹ï¼Œå†åŠ ä¸Šä»–é•¿æœŸç†¬å¤œå’Œä¹…åçš„å·¥ä½œä¹ æƒ¯ï¼Œè¿™äº›éƒ½å¯èƒ½æ˜¯å¯¼è‡´è¿™äº›ç—‡çŠ¶çš„åŸå› ã€‚å½“ç„¶ï¼Œæœ€å¥½è¿˜æ˜¯å»æ£€æŸ¥ä¸€ä¸‹è¡€å‹ï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯è¡€å‹çœŸçš„ä½ï¼Œæˆ–è€…æœ‰æ²¡æœ‰å…¶ä»–æ½œåœ¨çš„é—®é¢˜ã€‚\n",
      "\n",
      "å¦‚æœæ£€æŸ¥åå‘ç°è¡€å‹æ­£å¸¸ï¼Œé‚£å¯èƒ½éœ€è¦è€ƒè™‘å…¶ä»–å¯èƒ½æ€§ï¼Œæ¯”å¦‚å†…è€³é—®é¢˜æˆ–è€…é¢ˆæ¤ç—…ï¼Œæˆ–è€…å‹åŠ›å¼•èµ·çš„é—®é¢˜ã€‚æ€»ä¹‹ï¼Œä½è¡€å‹æ˜¯ä¸€ä¸ªæ¯”è¾ƒåˆç†çš„è§£é‡Šï¼Œä½†ä¹Ÿä¸èƒ½æ’é™¤å…¶ä»–å¯èƒ½ã€‚\n",
      "</think>\n",
      "æ ¹æ®æä¾›çš„ç—‡çŠ¶ï¼Œ28å²çš„ç”·æ€§ç¨‹åºå‘˜çªç„¶å‡ºç°å¤´æ™•ç›®çœ©ã€ç”šè‡³æ¶å¿ƒï¼Œè¿™ç§æƒ…å†µå¯èƒ½ä¸ä»¥ä¸‹å› ç´ æœ‰å…³ï¼š\n",
      "\n",
      "1. **ä½è¡€å‹æˆ–ä½“ä½æ€§ä½è¡€å‹**ï¼šé•¿æœŸç†¬å¤œå’Œä¹…åå¯èƒ½å¯¼è‡´è¡€æ¶²å¾ªç¯ä¸è‰¯ï¼Œä½“ä½æ€§ä½è¡€å‹åœ¨ç«™èµ·æ—¶ä¼šå› ä¸ºè¡€å‹çªç„¶ä¸‹é™è€Œå¯¼è‡´å¤´æ™•å’Œæ¶å¿ƒã€‚\n",
      "\n",
      "2. **é¢ˆæ¤ç—…**ï¼šç”±äºç¨‹åºå‘˜çš„å·¥ä½œç‰¹ç‚¹ï¼Œé•¿æœŸä¹…åå¯èƒ½å¯¼è‡´é¢ˆæ¤é—®é¢˜ï¼Œè¿›è€Œå¼•å‘å¤´æ™•å’Œæ¶å¿ƒã€‚\n",
      "\n",
      "3. **å‹åŠ›å’Œç„¦è™‘**ï¼šå·¥ä½œå‹åŠ›å¤§ã€ç¡çœ ä¸è¶³å¯èƒ½å¼•èµ·ç„¦è™‘ï¼Œè€Œç„¦è™‘æœ‰æ—¶ä¹Ÿä¼šå¯¼è‡´å¤´æ™•å’Œæ¶å¿ƒã€‚\n",
      "\n",
      "4. **ç”Ÿæ´»ä¹ æƒ¯**ï¼šç†¬å¤œã€ä¹…åå’Œé¥®é£Ÿä¸è§„å¾‹å¯èƒ½å½±å“æ•´ä½“å¥åº·ï¼Œå¯¼è‡´å¤´æ™•å’Œæ¶å¿ƒã€‚\n",
      "\n",
      "å»ºè®®è¯¥æ‚£è€…å°½å¿«å°±åŒ»ï¼Œé€šè¿‡è¡€å‹æµ‹é‡ã€é¢ˆæ¤æ£€æŸ¥æˆ–å…¶ä»–ç›¸å…³æ£€æŸ¥æ¥æ˜ç¡®ç—…å› ã€‚å¦‚æœè¡€å‹æ­£å¸¸ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥æ’æŸ¥å…¶ä»–æ½œåœ¨å› ç´ ã€‚åŒæ—¶ï¼Œæ”¹å–„ç”Ÿæ´»ä¹ æƒ¯ï¼Œå¦‚å¢åŠ è¿åŠ¨ã€ä¿è¯å……è¶³ç¡çœ å’Œå‡è¡¡é¥®é£Ÿï¼Œä¹Ÿæœ‰åŠ©äºç¼“è§£è¿™äº›ç—‡çŠ¶ã€‚\n"
     ]
    }
   ],
   "source": [
    "my_question = \"å¯¹äºä¸€å 28 å²çš„ç”·æ€§æ‚£è€…ï¼Œå·¥ä½œæ˜¯ç¨‹åºå‘˜ï¼Œå¸¸å¹´ç†¬å¤œï¼Œæœ€è¿‘çªç„¶æ„Ÿè§‰å¤´æ™•ç›®çœ©ï¼Œç”šè‡³æœ‰ç‚¹æ¶å¿ƒã€‚è¯·é—®æœ‰å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…ï¼Ÿ\"\n",
    "\n",
    "response = generate_response(my_question, model, tokenizer, inference_prompt)\n",
    "print(\"==================== æ¨¡å‹å›ç­” ====================\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:24:31.156749Z",
     "iopub.status.busy": "2025-08-31T14:24:31.156469Z",
     "iopub.status.idle": "2025-08-31T14:24:31.159472Z",
     "shell.execute_reply": "2025-08-31T14:24:31.158936Z",
     "shell.execute_reply.started": "2025-08-31T14:24:31.156729Z"
    }
   },
   "outputs": [],
   "source": [
    "### 10.åŠ è½½ 1.5b sftåçš„æ¨¡å‹è¿›è¡Œå¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:24:50.395910Z",
     "iopub.status.busy": "2025-08-31T14:24:50.395456Z",
     "iopub.status.idle": "2025-08-31T14:24:58.718474Z",
     "shell.execute_reply": "2025-08-31T14:24:58.717844Z",
     "shell.execute_reply.started": "2025-08-31T14:24:50.395886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.5: Fast Qwen2 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.5 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# è¿™ä¸€æ­¥ä¼šè¿”å›ä¸€ä¸ªç»è¿‡ Unsloth ä¼˜åŒ–çš„æ¨¡å‹å’Œä¸€ä¸ªåˆ†è¯å™¨\n",
    "sft_model_name_1_dot_5_b = 'qwen-1.5b_lora_model'\n",
    "model_sft_1_dot_5_b, tokenizer_sft_1_dot_5_b = FastLanguageModel.from_pretrained(\n",
    "    model_name = sft_model_name_1_dot_5_b,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:24:58.719999Z",
     "iopub.status.busy": "2025-08-31T14:24:58.719517Z",
     "iopub.status.idle": "2025-08-31T14:25:24.658950Z",
     "shell.execute_reply": "2025-08-31T14:25:24.658368Z",
     "shell.execute_reply.started": "2025-08-31T14:24:58.719978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== æ¨¡å‹å›ç­” ====================\n",
      "<think>\n",
      "å—¯ï¼Œè¿™ä½60å²çš„ç”·æ€§æ‚£è€…ï¼Œèƒ¸ç–¼ï¼ŒXçº¿æ˜¾ç¤ºå³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±ï¼Œè¿™è®©æˆ‘æƒ³åˆ°å¯èƒ½æœ‰è‚ºç»“æ ¸ã€‚è‚ºç»“æ ¸é€šå¸¸ä¼šå¯¼è‡´èƒ¸è…”ç§¯æ¶²ï¼Œå°¤å…¶æ˜¯åœ¨å³ä¾§ã€‚ç°åœ¨ï¼Œæˆ‘å¾—æƒ³æƒ³ï¼Œè¦äº†è§£èƒ¸æ°´çš„æ€§è´¨ï¼Œåº”è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿ\n",
      "\n",
      "é¦–å…ˆï¼Œèƒ¸æ°´çš„æ€§è´¨å¯èƒ½å’Œè‚ºç»“æ ¸çš„ç±»å‹æœ‰å…³ã€‚å¦‚æœæ˜¯è‚ºç»“æ ¸ï¼Œé€šå¸¸ä¼šæœ‰è‚ºéƒ¨çš„ç»“æ ¸èŒï¼Œè¿™äº›èŒä¼šåœ¨èƒ¸è…”å†…ç¹æ®–ï¼Œå¯¼è‡´ç§¯æ¶²ã€‚å¦‚æœè‚ºç»“æ ¸æ˜¯åŸå‘æ€§ï¼Œé‚£ä¹ˆèƒ¸è…”å†…çš„ç»“æ ¸èŒä¹Ÿä¼šç¹æ®–ï¼Œè¿™å¯èƒ½å½±å“èƒ¸æ°´çš„æ€§è´¨ã€‚\n",
      "\n",
      "é‚£ï¼Œæˆ‘éœ€è¦ä»€ä¹ˆæ£€æµ‹èƒ½å¸®åŠ©æˆ‘äº†è§£è¿™ä¸ªå‘¢ï¼Ÿå—¯ï¼Œå¦‚æœæˆ‘éœ€è¦äº†è§£èƒ¸æ°´çš„ç±»å‹ï¼Œæ¯”å¦‚æ˜¯å¦æ˜¯ç»“æ ¸èŒå¯¼è‡´çš„ï¼Œé‚£æˆ‘å¾—ç”¨æŠ—ç»“æ ¸ç´ æ£€æµ‹ã€‚è¿™å¯ä»¥å¸®åŠ©æˆ‘åˆ¤æ–­æ˜¯è‚ºç»“æ ¸è¿˜æ˜¯å…¶ä»–ç±»å‹çš„ç»“æ ¸ç—…ã€‚\n",
      "\n",
      "å¦‚æœè¦äº†è§£èƒ¸æ°´çš„ç»“æ„ï¼Œæ¯”å¦‚æ˜¯å¦æ˜¯ç»“æ ¸èŒå¼•èµ·çš„ï¼Œé‚£å¯èƒ½éœ€è¦ä¸€äº›è¶…å£°æˆ–CTæ‰«æï¼Œè¿™æ ·æˆ‘å¯ä»¥æ£€æŸ¥ä¸€ä¸‹èƒ¸è…”å†…çš„ç»“æ„ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œæœ€ç›´æ¥çš„æ–¹æ³•è¿˜æ˜¯å¾—å…ˆçŸ¥é“æ˜¯å¦æ˜¯è‚ºç»“æ ¸ã€‚å› ä¸ºè‚ºç»“æ ¸çš„èƒ¸è…”ç§¯æ¶²é€šå¸¸ä¼šå¼•å‘èƒ¸æ°´çš„æ€§è´¨å˜åŒ–ã€‚æ‰€ä»¥ï¼ŒæŠ—ç»“æ ¸ç´ æ£€æµ‹åº”è¯¥æ˜¯å…³é”®ï¼Œå®ƒå¯ä»¥å¸®åŠ©æˆ‘åˆ¤æ–­æ˜¯è‚ºç»“æ ¸è¿˜æ˜¯å…¶ä»–ç±»å‹çš„ç»“æ ¸ç—…ã€‚\n",
      "\n",
      "å½“ç„¶ï¼Œå¦‚æœæ€€ç–‘ä¸æ˜¯è‚ºç»“æ ¸ï¼Œä½†æœ‰å…¶ä»–åŸå› å¯¼è‡´èƒ¸è…”ç§¯æ¶²ï¼Œæˆ‘è¿˜éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ï¼Œæ¯”å¦‚è¡€å¸¸è§„ã€èƒ¸ç‰‡æˆ–CTï¼Œä»¥ç¡®è®¤ç—…å› ã€‚\n",
      "\n",
      "æ€»çš„æ¥è¯´ï¼ŒæŠ—ç»“æ ¸ç´ æ£€æµ‹æ˜¯æœ€ç›´æ¥çš„ï¼Œå› ä¸ºå®ƒèƒ½å¸®åŠ©æˆ‘ä»¬äº†è§£èƒ¸æ°´çš„æ€§è´¨ï¼Œä»è€ŒæŒ‡å¯¼è¿›ä¸€æ­¥çš„æ£€æŸ¥å’Œè¯Šæ–­ã€‚æ‰€ä»¥ï¼Œè¿™ä¸ªè¿‡ç¨‹åº”è¯¥å…ˆåšæŠ—ç»“æ ¸ç´ æ£€æµ‹ã€‚\n",
      "</think>\n",
      "å¯¹äºè¿™ä½60å²ç”·æ€§æ‚£è€…ï¼Œè‹¥Xçº¿æ£€æŸ¥æ˜¾ç¤ºå³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±å¹¶è¯Šæ–­ä¸ºè‚ºç»“æ ¸ä¼´å³ä¾§èƒ¸è…”ç§¯æ¶²ï¼ŒæŠ—ç»“æ ¸ç´ æ£€æµ‹æ˜¯æœ€ç›´æ¥çš„æ£€æŸ¥æ–¹æ³•ï¼Œå®ƒå¯ä»¥æä¾›å…³äºèƒ¸è…”ç§¯æ¶²çš„æ€§è´¨å’Œç±»å‹çš„ä¿¡æ¯ã€‚å¦‚æœæŠ—ç»“æ ¸ç´ æ£€æµ‹ç»“æœè¡¨æ˜æ˜¯è‚ºç»“æ ¸ï¼Œé‚£ä¹ˆèƒ¸è…”ç§¯æ¶²å¯èƒ½ä¸è‚ºéƒ¨ç»“æ ¸èŒæœ‰å…³ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œèƒ¸è…”ç§¯æ¶²çš„æ€§è´¨å¯èƒ½ä¸è‚ºç»“æ ¸èŒæœ‰å…³ã€‚ç›¸åï¼Œè‹¥æ£€æµ‹ç»“æœä¸æ˜ç¡®ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥çš„æ£€æŸ¥ä»¥ç¡®å®šç—…å› ã€‚å› æ­¤ï¼ŒæŠ—ç»“æ ¸ç´ æ£€æµ‹æ˜¯äº†è§£èƒ¸æ°´æ€§è´¨çš„æœ€æœ‰æ•ˆæ‰‹æ®µã€‚\n"
     ]
    }
   ],
   "source": [
    "# å¯¹æ¯”\n",
    "my_question = \"å¯¹äºä¸€å60å²ç”·æ€§æ‚£è€…ï¼Œå‡ºç°å³ä¾§èƒ¸ç–¼å¹¶åœ¨Xçº¿æ£€æŸ¥ä¸­æ˜¾ç¤ºå³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±ï¼Œè¯Šæ–­ä¸ºè‚ºç»“æ ¸ä¼´å³ä¾§èƒ¸è…”ç§¯æ¶²ï¼Œè¯·é—®å“ªä¸€é¡¹å®éªŒå®¤æ£€æŸ¥å¯¹äº†è§£èƒ¸æ°´çš„æ€§è´¨æ›´æœ‰å¸®åŠ©ï¼Ÿ\"\n",
    "\n",
    "response = generate_response(my_question, model_sft_1_dot_5_b, tokenizer_sft_1_dot_5_b, inference_prompt)\n",
    "print(\"==================== æ¨¡å‹å›ç­” ====================\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:25:24.660160Z",
     "iopub.status.busy": "2025-08-31T14:25:24.659636Z",
     "iopub.status.idle": "2025-08-31T14:25:47.823019Z",
     "shell.execute_reply": "2025-08-31T14:25:47.822448Z",
     "shell.execute_reply.started": "2025-08-31T14:25:24.660137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== æ¨¡å‹å›ç­” ====================\n",
      "<think>\n",
      "å“¦ï¼Œè¿™ä¸ª28å²çš„ç”·æ€§æ‚£è€…ï¼Œå·¥ä½œæ˜¯ç¨‹åºå‘˜ï¼Œç»å¸¸ç†¬å¤œï¼Œæœ€è¿‘åˆå‡ºç°äº†å¤´æ™•ç›®çœ©å’Œæ¶å¿ƒçš„ç—‡çŠ¶ã€‚å—¯ï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°å¯èƒ½æ˜¯è„‘åŠ›å‡é€€å§ï¼Œæ¯•ç«Ÿç¨‹åºå‘˜ç»å¸¸ç†¬å¤œï¼Œé•¿æœŸå·¥ä½œå‹åŠ›å¤§ã€‚\n",
      "\n",
      "è„‘åŠ›å‡é€€é€šå¸¸å’Œè„‘æŸä¼¤æœ‰å…³ï¼Œå°¤å…¶æ˜¯è„‘è†œç‚ã€‚è„‘è†œç‚æ˜¯è„‘éƒ¨çš„ç‚ç—‡ï¼Œé€šå¸¸ä¼šå¯¼è‡´ç–²åŠ³ã€æ¶å¿ƒå’Œå¤´æ™•ï¼Œè¿™äº›ç—‡çŠ¶æ­£æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„ã€‚ä¸è¿‡ï¼Œè„‘è†œç‚ä¹Ÿæœ‰ä¾‹å¤–ï¼Œæ¯”å¦‚è„‘è†œç‚çš„æŸäº›å¹¶å‘ç—‡ï¼Œæ¯”å¦‚è„‘è†œç‚æ€§è„‘æ°´è‚¿ï¼Œä¹Ÿä¼šå¯¼è‡´ä¸€äº›ç±»ä¼¼çš„ç—‡çŠ¶ã€‚\n",
      "\n",
      "ç°åœ¨ï¼Œæˆ‘éœ€è¦è€ƒè™‘è„‘è†œç‚å’Œè„‘è†œç‚æ€§è„‘æ°´è‚¿çš„å¯èƒ½æ€§ã€‚è„‘è†œç‚æ€§è„‘æ°´è‚¿æ˜¯ç”±äºè„‘è†œç‚å¼•å‘çš„ç¥ç»å…ƒæŸä¼¤ï¼Œå¯¼è‡´è¡€æ¶²ä¸­çš„ç¥ç»å†²åŠ¨è¢«æŠ‘åˆ¶ï¼Œä»è€Œå¼•èµ·ä¸€äº›å…¨èº«æ€§çš„ç—‡çŠ¶ï¼Œæ¯”å¦‚æ¶å¿ƒã€å¤´æ™•å’Œè‚Œè‚‰æ— åŠ›ï¼Œè¿™äº›ç—‡çŠ¶ä¹Ÿç¬¦åˆè¿™ä½æ‚£è€…çš„æƒ…å†µã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œå†ä»”ç»†æƒ³æƒ³ï¼Œè„‘è†œç‚é€šå¸¸ä¼šä¼´éšä¸€äº›ç¥ç»åŠŸèƒ½éšœç¢ï¼Œæ¯”å¦‚ä¸­æ¢ç¥ç»ç³»ç»ŸåŠŸèƒ½å‡é€€ï¼Œä½†æ˜¯è¿™äº›ç—‡çŠ¶ä¸»è¦æ˜¯ç”±äºè„‘è†œç‚å¼•èµ·çš„ï¼Œè€Œä¸ä»…ä»…æ˜¯è„‘è†œç‚æ€§è„‘æ°´è‚¿ã€‚\n",
      "\n",
      "è„‘è†œç‚æ€§è„‘æ°´è‚¿æ˜¯æ›´å…¸å‹çš„è„‘è†œç‚å¹¶å‘ç—‡ï¼Œå®ƒé€šå¸¸ä¸è„‘è†œç‚ä¸€èµ·å‡ºç°ï¼Œä½†å¹¶ä¸æ˜¯ç”±è„‘è†œç‚å•ç‹¬å¼•å‘çš„ã€‚å› æ­¤ï¼Œè™½ç„¶è„‘è†œç‚æ˜¯å¯èƒ½çš„åŸå› ä¹‹ä¸€ï¼Œä½†è„‘è†œç‚æ€§è„‘æ°´è‚¿æ›´ç¬¦åˆè¿™ä½æ‚£è€…çš„æƒ…å†µã€‚\n",
      "\n",
      "ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™ä½æ‚£è€…æœ€å¯èƒ½çš„ç–¾ç—…æ˜¯è„‘è†œç‚æ€§è„‘æ°´è‚¿ã€‚å½“ç„¶ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯è„‘è†œç‚æœ¬èº«ï¼Œä½†è„‘è†œç‚æ€§è„‘æ°´è‚¿æ›´ç¬¦åˆæ‚£è€…çš„ç—‡çŠ¶å’Œæƒ…å†µã€‚\n",
      "</think>\n",
      "è¿™ä½28å²çš„ç”·æ€§æ‚£è€…ï¼Œå·¥ä½œæ˜¯ç¨‹åºå‘˜ï¼Œé•¿æœŸç†¬å¤œï¼Œæœ€è¿‘å‡ºç°å¤´æ™•ç›®çœ©å’Œæ¶å¿ƒç­‰ç—‡çŠ¶ï¼Œæœ€å¯èƒ½çš„ç–¾ç—…æ˜¯è„‘è†œç‚æ€§è„‘æ°´è‚¿ã€‚è„‘è†œç‚é€šå¸¸ä¼šå¯¼è‡´è„‘éƒ¨çš„ç‚ç—‡ï¼Œä»è€Œå¼•èµ·å¤´æ™•ç›®çœ©å’Œæ¶å¿ƒç­‰å…¨èº«æ€§ç—‡çŠ¶ã€‚è„‘è†œç‚æ€§è„‘æ°´è‚¿åˆ™æ˜¯ç”±äºè„‘è†œç‚å¼•å‘çš„ç¥ç»å…ƒæŸä¼¤ï¼Œå¯¼è‡´è¡€æ¶²ä¸­çš„ç¥ç»å†²åŠ¨è¢«æŠ‘åˆ¶ï¼Œä»è€Œå¼•èµ·ä¸€äº›å…¨èº«æ€§çš„ç—‡çŠ¶ï¼Œå¦‚æ¶å¿ƒã€å¤´æ™•å’Œè‚Œè‚‰æ— åŠ›ã€‚å› æ­¤ï¼Œè„‘è†œç‚æ€§è„‘æ°´è‚¿æ˜¯æœ€ç¬¦åˆæ‚£è€…ç—‡çŠ¶å’Œæƒ…å†µçš„ç–¾ç—…ã€‚\n"
     ]
    }
   ],
   "source": [
    "# å¯¹æ¯”\n",
    "my_question = \"å¯¹äºä¸€å 28 å²çš„ç”·æ€§æ‚£è€…ï¼Œå·¥ä½œæ˜¯ç¨‹åºå‘˜ï¼Œå¸¸å¹´ç†¬å¤œï¼Œæœ€è¿‘çªç„¶æ„Ÿè§‰å¤´æ™•ç›®çœ©ï¼Œç”šè‡³æœ‰ç‚¹æ¶å¿ƒã€‚è¯·é—®æœ‰å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…ï¼Ÿ\"\n",
    "\n",
    "response = generate_response(my_question, model_sft_1_dot_5_b, tokenizer_sft_1_dot_5_b, inference_prompt)\n",
    "print(\"==================== æ¨¡å‹å›ç­” ====================\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
